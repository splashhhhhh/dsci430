{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f416c434",
   "metadata": {},
   "source": [
    "# DSCI 430 - Fairness, Accountability, Transparency and Ethics (FATE) in Data Science\n",
    "\n",
    "# Module 1 - Introduction and Ethical foundations \n",
    "\n",
    "\n",
    "\n",
    "### Assignment overview\n",
    "\n",
    "This assignment is composed of three parts:\n",
    "- **Part 1 - The Black Mirror Writers' Room.** In this portion of the exercise, you will brainstorm near future technology and its possible drawbacks, and illustrate them in a futuristic cautionary tale. You will also be asked questions about ethical theories and how they apply to the scenario you have described. Credits: [Casey Fiesler - The Black Mirror Writers Room: The Case (and Caution) for Ethical Speculation in CS Education](https://medium.com/cuinfoscience/the-black-mirror-writers-room-the-case-and-caution-for-ethical-speculation-in-cs-education-5c81d05d2c67)\n",
    "- **Part 2 - Python review.** As this course uses Python as the programming language for our exercises, a basic understanding of the fundamentals and the use of some libraries is necessary. This portion of the exercise will help you review useful Python syntax and/or fill the gap in your knowledge before tackling larger exercises. We recommend discussing with an instructor if you find this portion of the assignment too difficult to complete with a reasonable amount of effort.\n",
    "- **Part 3 - Final thoughts.** Complete this section so that we can better understand how you completed the assignment and any issues you may have encountered.\n",
    "\n",
    "For this assignment, it is possible to work in **groups of up to 2 students**. Read the instructions carefully, as they may assign tasks to specific students.\n",
    "\n",
    "### Group members\n",
    "Leave Student 2 blank if group has less than 2 members:\n",
    "- Student 1: Muhan Yang\n",
    "- Student 2:\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "After completing this week's lecture and tutorial work, you will be able to:\n",
    "1. Define ethics and describe what constitutes an ethical issue \n",
    "2. Explain the need for ethics in data science  \n",
    "3. Identify common ethical issues in data science  \n",
    "4. Describe common ethical frameworks and how they can be applied to data science applications \n",
    "5. Imagine scenarios in which current technology could be used in unethical ways \n",
    "6. Evaluate and make arguments around data science scenarios using ethical theories (e.g., Kantianism, utilitarianism, virtue ethics etc.)  \n",
    "7. Compare and contrast different ethical theories and explain the case for and against each one as they apply to data science\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522ae92",
   "metadata": {},
   "source": [
    "# The Black Mirror Writers' Room\n",
    "\n",
    "Black Mirror is a Netflix series centered around the use of advanced technology and its possible unexpected (sometimes catastrophic) consequences. In this exercise, you will come up with your very own Black Mirror episode (or at least a synopsis)! \n",
    "\n",
    "## Warm up\n",
    "\n",
    "Before jumping into the creative writing part, we should review the various elements of FATE in Data Science and make sure that they are clear:\n",
    "\n",
    "| FATE element    | Definition |\n",
    "| :-------- | :------- |\n",
    "| **Fairness**  | The idea that every group or population that is affected by a technological application is being treated equally and not receiving a different outcome *solely because they belong to their group*.   |\n",
    "| **Accountability** | Clear definition of who should be held responsible of the outcome of the technological application and under what circumstances. |\n",
    "| **Transparency** | The technical definition of transparency in Data Science refers to being able to understand why a technological application produced a specific outcome. This is also called *explainability*. But transparency can also refer to the demand of making the use of algorithms more transparent to the public, including informing the users about when they are used, where the data used was sourced from, and making algorithms available for auditing.|\n",
    "| **Ethics** | Evaluation of whether or not a technology should be used based on the moral values of a group or society. Society may reject a technology because it is does not follow the principles of Fairness, Accountability or Transparency, but also for other reasons.|\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "In the country of Dataland, the police department uses an algorithm to assess the risk level of people reporting cases of domestic abuses and violence. Thanks to this algorithm, they can identify the most serious threats and intervene accordingly. The algorithm has had a positive impact, assessing cases with more accuracy than other prior strategies and allowing the police force to make an efficient use of their resources. However, it occasionally fails to correctly identify people at high risk of violence (*false negatives*), leaving them without the protection they need. It is also affected by other issues. For each issue outlined in this table, check whether it is a Fairness, Accountability or Transparency problem.\n",
    "\n",
    "| Issue    | Fairness | Accountability | Transparency |\n",
    "| :-------- | :------- | :------- | :------- |\n",
    "| When the algorithm fails to identify a high-risk case and violence occurs, it is unclear if the police department should shoulder any responsibility. |        |✅|        | \n",
    "| An analysis of the algorithm's results suggests that false negatives occur more frequently among victims with physical disabilities. |✅|        |        | \n",
    "| The majority of people reporting domestic abuse are not aware that their cases are being evaluated by an algorithm, or do not know the score they received. |        |        |✅| \n",
    "| The police department receives a recommendation for each case, but does not know which characteristic(s) of the case have resulted in the final evaluation. |        |        |✅| \n",
    "| The algorithm was trained using past cases filed by the police department, but the people involved where not informed that their information was being used for this purpose. |        |        |✅| \n",
    "\n",
    "### Question 2\n",
    "\n",
    "Considere the issues outlined in the previous question, as well as the fact that the algorithm is the best system of appraisal available to the police forces so far for cases of domestic violence. Do you think that the use of this algorithm is *ethical*? Clearly state your thesis (opposed/favourable) and use one of the ethical perspectives listed in [this reading](https://www.scu.edu/ethics-in-technology-practice/ethical-lenses/) to support it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d70f1",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c719b",
   "metadata": {},
   "source": [
    "**Note:** this case is fictional but inspired by a real algorithm, called VioGén, used in Spain to determine the risk level of victims of gender-based violence and assign protection measures. The algorithm has been recently going under severe scrutiny [(Read more).](https://eticas.ai/the-adversarial-audit-of-viogen-three-years-later/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716871c",
   "metadata": {},
   "source": [
    "### Question 3: Write your own Black Mirror episode\n",
    "\n",
    "Now that you have acquired the necessary familiarity with some required knowledge and terminology, it is time to use your creativity!\n",
    "\n",
    "**Step 1:** Brainstorm *one* near future technology based on a topic of your choice. It should be close enough that it seems like a plausible future. Describe it in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422e4c9",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915ce50",
   "metadata": {},
   "source": [
    "**Step 2:** What are the potential social implications and/or ethical issues and/or regulatory challenges with this technology? Explain if and how they are connected to FATE (e.g. is it a Fairness issue? Or maybe a Transparency issue? It could be more than one option)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24479bf",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecd0e0",
   "metadata": {},
   "source": [
    "**Step 3:** Time for storytelling! Write the summary of a new Black Mirror episode based on the technology of your choice. Try covering all of the following:\n",
    "\n",
    "- What do you think might be a cautionary tale related to this technology? \n",
    "- What fictional person in the future would best illustrate this caution? Provide a detailed description and explain what makes them the best character to carry your message.\n",
    "- What is their story? Explain their background, their motivations, and their journey through your episode.\n",
    "\n",
    "As part of your submission, please update the episode thumbnail slide (from Module 1 slide deck) using information from your episode. Don't forget to add a picture! Then, share it with the rest of the class on [Canvas](https://canvas.ubc.ca/courses/134264/discussion_topics/1943920).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b3d7b",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c018b",
   "metadata": {},
   "source": [
    "**Step 4:** Let's take a step back, and imagine that you are (one of) an activist/legistlator/technology practitioner at the time the technology described in your episode is being developed and its use being discussed. Select *one* of the ethical perspectives listed in [this reading](https://www.scu.edu/ethics-in-technology-practice/ethical-lenses/), and use this perspective to argue against its deployment. Pay particular attention to the counter-arguments! People with interests in this technology will certainly argue against you, and you must anticipate and rebut their claims. Include at least 2 conter-arguments and how you would respond to them.\n",
    "\n",
    "**Ethical perspective chosen:** \n",
    "\n",
    "**Argument:**\n",
    "\n",
    "**First counter-argument (with rebuttal):**\n",
    "\n",
    "**Second counter-argument (with rebuttal):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a531a70",
   "metadata": {},
   "source": [
    "**Step 5:** Finally, let's end on a more positive note and imagine a \"Light Mirror\" scenario, where the negative consequences of the technology you have described are averted and positive results are achieved in their stead. Try answering the following questions:\n",
    "\n",
    "1. What kinds of solutions can be deployed in the immediate for addressing the harms of the technology you have described? What could we do to ensure that we don’t get to the negative consequences you imagined later in the future?\n",
    "2. Could you imagine a scenario where the technology you have described is used with positive consequences, given the appropriate safe guards? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa8a9e",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44908a97",
   "metadata": {},
   "source": [
    "### Sources\n",
    "The Black Mirror Writers' Room exercises was designed by [Dr. Casey Fiesler](https://caseyfiesler.com/). Links to her work and publications:\n",
    "- [The Black Mirror Writers Room: The Case (and Caution) for Ethical Speculation in CS Education](https://medium.com/cuinfoscience/the-black-mirror-writers-room-the-case-and-caution-for-ethical-speculation-in-cs-education-5c81d05d2c67)\n",
    "- [\"Run Wild a Little With Your Imagination\": Ethical Speculation in Computing Education with Black Mirror](https://dl.acm.org/doi/abs/10.1145/3478431.3499308)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2f627",
   "metadata": {},
   "source": [
    "# Python Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f514",
   "metadata": {},
   "source": [
    "In this section of the assignment, we will review useful Python functions and libraries that will allow you to read and analyze data, as well as training simple Machine Learning models. \n",
    "\n",
    "## Section 1: Exploring datasets with Pandas\n",
    "\n",
    "First, we will need a dataset to work on. Let's use a [weather type dataset](https://www.kaggle.com/datasets/nikhil7280/weather-type-classification), a good starting dataset (we will save more interesting cases for later!). Download this dataset from the link to use it for this exercise.\n",
    "\n",
    "We also need to import the necessary library to read and manipulate our dataset, which is [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html). The imports are given to you. Next, use the `read_csv()` function to import the data in your workspace. The documentation for this function can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cc57bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n",
      "0         14.0        73         9.5               82.0  partly cloudy   \n",
      "1         39.0        96         8.5               71.0  partly cloudy   \n",
      "2         30.0        64         7.0               16.0          clear   \n",
      "3         38.0        83         1.5               82.0          clear   \n",
      "4         27.0        74        17.0               66.0       overcast   \n",
      "\n",
      "   Atmospheric Pressure  UV Index  Season  Visibility (km)  Location  \\\n",
      "0               1010.82         2  Winter              3.5    inland   \n",
      "1               1011.43         7  Spring             10.0    inland   \n",
      "2               1018.72         5  Spring              5.5  mountain   \n",
      "3               1026.25         7  Spring              1.0   coastal   \n",
      "4                990.67         1  Winter              2.5  mountain   \n",
      "\n",
      "  Weather Type  \n",
      "0        Rainy  \n",
      "1       Cloudy  \n",
      "2        Sunny  \n",
      "3        Sunny  \n",
      "4        Rainy  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"weather_classification_data.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59a2ab",
   "metadata": {},
   "source": [
    "Now, let's use the [`describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function of the Pandas library to get an overview of the dataset, and answer the following questions (you can write your answers in this box):\n",
    "\n",
    "- What is the maximum temperature recorded in the dataset? *109*\n",
    "- What is the average wind speed? *9.83*\n",
    "\n",
    "Note: some of the values you will see may appear unrealistic (such as incredibly high temperatures). The dataset is artificially generated and purposefully includes outliers to practice detection and handling, but it is not something we will worry about it this exercise - we are just interested in getting some practice with useful commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3ab972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Temperature      Humidity    Wind Speed  Precipitation (%)  \\\n",
      "count  13200.000000  13200.000000  13200.000000       13200.000000   \n",
      "mean      19.127576     68.710833      9.832197          53.644394   \n",
      "std       17.386327     20.194248      6.908704          31.946541   \n",
      "min      -25.000000     20.000000      0.000000           0.000000   \n",
      "25%        4.000000     57.000000      5.000000          19.000000   \n",
      "50%       21.000000     70.000000      9.000000          58.000000   \n",
      "75%       31.000000     84.000000     13.500000          82.000000   \n",
      "max      109.000000    109.000000     48.500000         109.000000   \n",
      "\n",
      "       Atmospheric Pressure      UV Index  Visibility (km)  \n",
      "count          13200.000000  13200.000000     13200.000000  \n",
      "mean            1005.827896      4.005758         5.462917  \n",
      "std               37.199589      3.856600         3.371499  \n",
      "min              800.120000      0.000000         0.000000  \n",
      "25%              994.800000      1.000000         3.000000  \n",
      "50%             1007.650000      3.000000         5.000000  \n",
      "75%             1016.772500      7.000000         7.500000  \n",
      "max             1199.210000     14.000000        20.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985c0a8",
   "metadata": {},
   "source": [
    "The `describe()` function is helpful, but it does not answer all the questions we may have. For example, we did not get any idea about the class distribution in our dataset, that is, how many samples we have for each of the four classes (Rainy, Cloudy, Sunny, Snowy). Can you write a line of code to answer this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a752e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              count\n",
      "Weather Type       \n",
      "Rainy          3300\n",
      "Cloudy         3300\n",
      "Sunny          3300\n",
      "Snowy          3300\n"
     ]
    }
   ],
   "source": [
    "print(data[\"Weather Type\"].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ababd",
   "metadata": {},
   "source": [
    "Thanks to `describe()`, we know that the minimum temperature recorded is -25 C, but we have no idea which sample it belongs to. Can you write a line of code to find the sample number and also the Weather Type associated to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9f73283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4609 Sunny\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[data['Temperature'] == -25, ['Weather Type']].index[0], data.loc[data['Temperature'] == 30, 'Weather Type'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346446a",
   "metadata": {},
   "source": [
    "Again thanks to `describe()`, we know that 25% of the samples in the dataset have a recorded Precipitation higher that 82 (you can verify this in the output table), but how many of these are Snowy? Answer this question in 1 line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ebea834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243\n"
     ]
    }
   ],
   "source": [
    "print(len(data[(data['Precipitation (%)'] > 82)&(data['Weather Type']=='Snowy')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75a00a",
   "metadata": {},
   "source": [
    "Finally, sometimes we may be interested in sorting the dataframe by the values in a column. In this cell, sort the dataframe by humidity in descending order, and check the results by printing the first 5 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07aaa619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n",
      "1303          29.0       109        21.0               93.0  partly cloudy   \n",
      "8716          16.0       109        27.0              102.0       overcast   \n",
      "9707          51.0       109        17.0               98.0       overcast   \n",
      "2812          16.0       109        39.0               87.0  partly cloudy   \n",
      "12566          4.0       109        16.0               93.0       overcast   \n",
      "\n",
      "       Atmospheric Pressure  UV Index  Season  Visibility (km) Location  \\\n",
      "1303                1018.98         9  Winter              7.5   inland   \n",
      "8716                1007.30         1  Winter             11.5   inland   \n",
      "9707                 994.03         8  Spring              5.5  coastal   \n",
      "2812                1011.38        11  Spring              2.0   inland   \n",
      "12566                988.15        12  Winter              3.5   inland   \n",
      "\n",
      "      Weather Type  \n",
      "1303        Cloudy  \n",
      "8716        Cloudy  \n",
      "9707         Rainy  \n",
      "2812         Rainy  \n",
      "12566        Snowy  \n"
     ]
    }
   ],
   "source": [
    "print(data.sort_values('Humidity', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f01f7e",
   "metadata": {},
   "source": [
    "As last step of this section, save the sorted dataframe in a new csv file called \"weather_data_by_humidity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c81a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('Humidity', ascending=False).to_csv('weather_data_by_humidity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097333c3",
   "metadata": {},
   "source": [
    "## Section 2: Training ML models with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b407cb",
   "metadata": {},
   "source": [
    "We are now interested in creating a model to predict the weather type based on the features available. Let's see how to do that using the python library [Scikit-learn](https://scikit-learn.org/stable/), while reviewing some important concepts about training and evaluating models. Simply run the cells below to see the output and answer the related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cefd1",
   "metadata": {},
   "source": [
    "First, we need to split our data set into training and testing set. The next cell shows how to do that. We will also separate the Weather Type column (target) from the other columns (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9133334b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Cloud Cover</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Season</th>\n",
       "      <th>Visibility (km)</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12987</th>\n",
       "      <td>26.0</td>\n",
       "      <td>45</td>\n",
       "      <td>3.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1011.01</td>\n",
       "      <td>7</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>5.0</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>29.0</td>\n",
       "      <td>71</td>\n",
       "      <td>21.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>1013.77</td>\n",
       "      <td>12</td>\n",
       "      <td>Winter</td>\n",
       "      <td>6.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5590</th>\n",
       "      <td>38.0</td>\n",
       "      <td>63</td>\n",
       "      <td>5.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1013.87</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>7.5</td>\n",
       "      <td>mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>17.0</td>\n",
       "      <td>66</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>992.22</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>32.0</td>\n",
       "      <td>39</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1021.43</td>\n",
       "      <td>9</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>5.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n",
       "12987         26.0        45         3.5               10.0          clear   \n",
       "905           29.0        71        21.0               86.0  partly cloudy   \n",
       "5590          38.0        63         5.5               11.0          clear   \n",
       "7269          17.0        66        18.0               63.0  partly cloudy   \n",
       "1417          32.0        39         7.5                3.0          clear   \n",
       "\n",
       "       Atmospheric Pressure  UV Index  Season  Visibility (km)  Location  \n",
       "12987               1011.01         7  Autumn              5.0    inland  \n",
       "905                 1013.77        12  Winter              6.5    inland  \n",
       "5590                1013.87        11  Spring              7.5  mountain  \n",
       "7269                 992.22         1  Winter              2.5    inland  \n",
       "1417                1021.43         9  Autumn              5.5    inland  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=123)  # 80%-20% train test split on df\n",
    "\n",
    "X_train, y_train = train_df.drop(columns=[\"Weather Type\"]), train_df[\"Weather Type\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"Weather Type\"]), test_df[\"Weather Type\"]\n",
    "\n",
    "X_train.head()  # quick visual check on X_train, the features dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91b843",
   "metadata": {},
   "source": [
    "**Question for you:** creating a testing set is very important when training a model. **Why? How is it used? What would happen if we did not do this very important step?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab435e4",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cf61e",
   "metadata": {},
   "source": [
    "As you can see, the dataset includes categorical features. Most classifiers require categorical features to be transformed before they can be used for training and prediction. The code below uses [One Hot Encoding](https://www.geeksforgeeks.org/ml-one-hot-encoding/) to convert the categorical features Cloud Cover, Season and Location, while leaving the numberical features unchanged.\n",
    "\n",
    "This is a simple example of data preprocessing. Preprocessing can be more extensive (for example, including [scaling of numerical features](https://www.geeksforgeeks.org/ml-feature-scaling-part-2/)), but we are only interested in an overview of the fundamentals, so we will just apply One Hot Encoding to make the data usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e61d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "passthrough = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', \n",
    "               'Atmospheric Pressure', 'UV Index', 'Visibility (km)']\n",
    "categorical = ['Cloud Cover', 'Season', 'Location']\n",
    "\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough),  # no transformations on the numberical features\n",
    ")\n",
    "\n",
    "# Fit the encoder on the training data and transform\n",
    "train_encoded = ct.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "test_encoded = ct.transform(X_test)\n",
    "\n",
    "# Convert the encoded data back to DataFrame for better readability\n",
    "\n",
    "column_names = (\n",
    "    ct.named_transformers_[\"onehotencoder\"].get_feature_names_out().tolist() + passthrough\n",
    ")\n",
    "\n",
    "X_train_encoded = pd.DataFrame(train_encoded, columns=column_names)\n",
    "X_test_encoded = pd.DataFrame(test_encoded, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a98bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see what the encoded data set looks like\n",
    "\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a80f0",
   "metadata": {},
   "source": [
    "**Question for you:** It appears that we applied the same One Hot Encoding transformation to both training and test set. Why did we bother doing this operation on the separate sets? Could have we just transformed the original dataframe `df`, and then split it in training and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca55587",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcfe025",
   "metadata": {},
   "source": [
    "There are many classifiers we can choose from. We will use [Decision Trees](https://scikit-learn.org/stable/modules/svm.html) to start. Decision trees are very simple classification algorithms, although they have typically mediocre performance on complex classification problems.\n",
    "\n",
    "A certain level of familiarity with Decision Trees is expected in this course. You may want to review the material from your previous courses, or this [introduction](https://www.youtube.com/watch?v=ZVR2Way4nwQ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d713bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier() # Create a decision tree\n",
    "model.fit(X_train_encoded, y_train) # Fit a decision tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404049a",
   "metadata": {},
   "source": [
    "Now that we have the tree, we want to see how well it performs. Let's first check the accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05252e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train_encoded, y_train) # Score the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4344e6",
   "metadata": {},
   "source": [
    "**100% accuracy!!!**\n",
    "\n",
    "...\n",
    "\n",
    "This sounds too good to be true... let's check the test set to see how well the trees perform on unseen samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023977ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test_encoded, y_test) # Score the decision tree on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de26d80",
   "metadata": {},
   "source": [
    "Accuracy dropped significantly when we moved to unseen samples!\n",
    "\n",
    "This is because the Decision Tree, if left unsupervised, is very prone to **overfitting**. \n",
    "\n",
    "**Question for you:** what does it mean for a model to overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5f27c",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b305fe8",
   "metadata": {},
   "source": [
    "To prevent a model from overfitting, we tune its **hyperparameters.** Hyperparameters are like knobs that we can use to regulate the way a model learn. \n",
    "\n",
    "Some hyperparameters for the scikit-learn DecisionTreeClassifier include:\n",
    "- max_depth: the maximum distance between the root node and a leaf node\n",
    "- min_samples_split: the minimum number of samples required to split an internal node\n",
    "- min_samples_leaf; the minimum number of samples required to be at a leaf node\n",
    "\n",
    "You can look up other hyperparameters and their default values in the DecisionTreeClassifier [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). By default, the maximum depth value is set to *None*, that is, the tree is free to grow until it has parfectly classified all samples. As we have seen, this results in perfect accuracy on the training set, but much lower accuracy on unseen samples. \n",
    "\n",
    "Run the cell below to see the depth of our overfitted tree:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a045a20",
   "metadata": {},
   "source": [
    "If we could find the right depth for our tree, we could reduce the problem of overfitting.\n",
    "\n",
    "**Question for you:** what would happen if we reduce the depth of the tree *too much*? What do you expect the accuracy on training and test set to look like in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21765a",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b918627",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is typically done on a **validation set.** A validation set is a set of samples not used for training, like the test set, but unlike the test set, we are allowed to use this multiple times as we look for the best hyperparameter values.\n",
    "\n",
    "Because our data set is rather small, it is not great to take more samples from the training set to create a validation set, because:\n",
    "- We would have fewer samples (less information) to train our model\n",
    "- The validation set would also be small, and result in a highly variable accuracy measure (meaning if we run the experiment again changing the samples in each set, we will likely get very different results)\n",
    "\n",
    "There is a method that we can use to eliminate both problems, called ***k*-fold cross-validation.** Cross-validation iteratively separates training and validation set (*k* times), so we get multiple measures of accuracy on the validation sets, which can be averaged for a more stable result. A good understanding of how cross-validation works is important for any data scientist. I encourage you to review cross-validation from previous courses, or this [introduction video](https://www.youtube.com/watch?v=4cv8VYonepA) (courtesy of Dr. Kolhatkar).\n",
    "\n",
    "Scikit-learn has a great method that we can use to perform cross-validation and find the best hyperparameters for a model at the same time, called [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). Let's use it to find the best depth for our Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np  # to create the array of values for depth\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": np.arange(1, 20, 1)  # testing all depths from 1 to 19\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, param_grid, cv=10, n_jobs=-1, return_train_score=True   # 10-fold cross-validation for all possible \n",
    "                                                                   # depths\n",
    ")\n",
    "grid_search.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4668c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f4143",
   "metadata": {},
   "source": [
    "**Complete the sentence (replace --?--):** Among all possible trees, GridSearchCV picked a tree of depth **--?--**, with an average validation accuracy of **--?--**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc8999",
   "metadata": {},
   "source": [
    "The accuracy on the training set is no longer 100%, but we expect this tree to perform better on unseen samples. Let's try it on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "best_tree.score(X_test_encoded, y_test) # Score the decision tree on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The accuracy is similar to when the model was overfitting, but hyperparameter tuning brought us 2 advantages:\n",
    "- We had a more realistic expectation of what our accuracy was going to be (closer to 91%, not 100%)\n",
    "- We simplified the model and reduced its depth. This makes the model faster and easier to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5209b3",
   "metadata": {},
   "source": [
    "**Question for you:** on what samples (or portion of samples) of `X_train_encoded` was the final model (`best_tree`) trained on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdded8",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc9bf",
   "metadata": {},
   "source": [
    "The model can now be used to get predictions for unseen samples. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = X_test_encoded.sample(n=1, random_state=42)\n",
    "\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c18eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree.predict(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a95af",
   "metadata": {},
   "source": [
    "# Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadac478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1) If you have completed this assignment in a group, please write a detailed description of how you divided the work and how you helped each other completing it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba472e70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c81ffd",
   "metadata": {},
   "source": [
    "2) Have you used ChatGPT or a similar Large Language Model (LLM) to complete this homework? Please describe how you used the tool. **We will never deduct points for using LLMs for completing homework assignments,** but this helps us understand how you are using the tool and advise you in case we believe you are using it incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fa1e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08a7bf06",
   "metadata": {},
   "source": [
    "3) Have you struggled with some parts (or all) of this homework? Do you have pending questions you would like to ask? Write them down here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b82cbc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci430]",
   "language": "python",
   "name": "conda-env-dsci430-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
