{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f416c434",
   "metadata": {},
   "source": [
    "# DSCI 430 - Fairness, Accountability, Transparency and Ethics (FATE) in Data Science\n",
    "\n",
    "# Module 1 - Introduction and Ethical foundations \n",
    "\n",
    "\n",
    "\n",
    "### Assignment overview\n",
    "\n",
    "This assignment is composed of three parts:\n",
    "- **Part 1 - The Black Mirror Writers' Room.** In this portion of the exercise, you will brainstorm near future technology and its possible drawbacks, and illustrate them in a futuristic cautionary tale. You will also be asked questions about ethical theories and how they apply to the scenario you have described. Credits: [Casey Fiesler - The Black Mirror Writers Room: The Case (and Caution) for Ethical Speculation in CS Education](https://medium.com/cuinfoscience/the-black-mirror-writers-room-the-case-and-caution-for-ethical-speculation-in-cs-education-5c81d05d2c67)\n",
    "- **Part 2 - Python review.** As this course uses Python as the programming language for our exercises, a basic understanding of the fundamentals and the use of some libraries is necessary. This portion of the exercise will help you review useful Python syntax and/or fill the gap in your knowledge before tackling larger exercises. We recommend discussing with an instructor if you find this portion of the assignment too difficult to complete with a reasonable amount of effort.\n",
    "- **Part 3 - Final thoughts.** Complete this section so that we can better understand how you completed the assignment and any issues you may have encountered.\n",
    "\n",
    "For this assignment, it is possible to work in **groups of up to 2 students**. Read the instructions carefully, as they may assign tasks to specific students.\n",
    "\n",
    "### Group members\n",
    "Leave Student 2 blank if group has less than 2 members:\n",
    "- Student 1: Ayuho Negishi\n",
    "- Student 2: Muhan Yang\n",
    "\n",
    "### Learning Goals:\n",
    "\n",
    "After completing this week's lecture and tutorial work, you will be able to:\n",
    "1. Define ethics and describe what constitutes an ethical issue \n",
    "2. Explain the need for ethics in data science  \n",
    "3. Identify common ethical issues in data science  \n",
    "4. Describe common ethical frameworks and how they can be applied to data science applications \n",
    "5. Imagine scenarios in which current technology could be used in unethical ways \n",
    "6. Evaluate and make arguments around data science scenarios using ethical theories (e.g., Kantianism, utilitarianism, virtue ethics etc.)  \n",
    "7. Compare and contrast different ethical theories and explain the case for and against each one as they apply to data science\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522ae92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The Black Mirror Writers' Room\n",
    "\n",
    "Black Mirror is a Netflix series centered around the use of advanced technology and its possible unexpected (sometimes catastrophic) consequences. In this exercise, you will come up with your very own Black Mirror episode (or at least a synopsis)! \n",
    "\n",
    "## Warm up\n",
    "\n",
    "Before jumping into the creative writing part, we should review the various elements of FATE in Data Science and make sure that they are clear:\n",
    "\n",
    "| FATE element    | Definition |\n",
    "| :-------- | :------- |\n",
    "| **Fairness**  | The idea that every group or population that is affected by a technological application is being treated equally and not receiving a different outcome *solely because they belong to their group*.   |\n",
    "| **Accountability** | Clear definition of who should be held responsible of the outcome of the technological application and under what circumstances. |\n",
    "| **Transparency** | The technical definition of transparency in Data Science refers to being able to understand why a technological application produced a specific outcome. This is also called *explainability*. But transparency can also refer to the demand of making the use of algorithms more transparent to the public, including informing the users about when they are used, where the data used was sourced from, and making algorithms available for auditing.|\n",
    "| **Ethics** | Evaluation of whether or not a technology should be used based on the moral values of a group or society. Society may reject a technology because it is does not follow the principles of Fairness, Accountability or Transparency, but also for other reasons.|\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "In the country of Dataland, the police department uses an algorithm to assess the risk level of people reporting cases of domestic abuses and violence. Thanks to this algorithm, they can identify the most serious threats and intervene accordingly. The algorithm has had a positive impact, assessing cases with more accuracy than other prior strategies and allowing the police force to make an efficient use of their resources. However, it occasionally fails to correctly identify people at high risk of violence (*false negatives*), leaving them without the protection they need. It is also affected by other issues. For each issue outlined in this table, check whether it is a Fairness, Accountability or Transparency problem.\n",
    "\n",
    "| Issue    | Fairness | Accountability | Transparency |\n",
    "| :-------- | :------- | :------- | :------- |\n",
    "| When the algorithm fails to identify a high-risk case and violence occurs, it is unclear if the police department should shoulder any responsibility. |        |   ○    |        | \n",
    "| An analysis of the algorithm's results suggests that false negatives occur more frequently among victims with physical disabilities. |   ○    |        |        | \n",
    "| The majority of people reporting domestic abuse are not aware that their cases are being evaluated by an algorithm, or do not know the score they received. |        |        |    ○   | \n",
    "| The police department receives a recommendation for each case, but does not know which characteristic(s) of the case have resulted in the final evaluation. |        |        |     ○   | \n",
    "| The algorithm was trained using past cases filed by the police department, but the people involved where not informed that their information was being used for this purpose. |        |        |     ○   | \n",
    "\n",
    "### Question 2\n",
    "\n",
    "Considere the issues outlined in the previous question, as well as the fact that the algorithm is the best system of appraisal available to the police forces so far for cases of domestic violence. Do you think that the use of this algorithm is *ethical*? Clearly state your thesis (opposed/favourable) and use one of the ethical perspectives listed in [this reading](https://www.scu.edu/ethics-in-technology-practice/ethical-lenses/) to support it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d70f1",
   "metadata": {},
   "source": [
    "*I am opposed to the thesis because it goes against the Utilitarian perspective, which emphasizes on the overall happiness, the balance of interests, and the consequences. This perspective holds that an action is ethical as long as it increases the happiness and welfare of all those people involved. However, the algorithm \"occasionally fails to correctly identify people at high risk of violence (false negatives)\", and those categorized as false negatives are unable to receive the necessary protection. Therefore, the use of this algorithm does not guarantee the happiness or welfare of everyone and fails to satisfy the requirements of this perspective. Also, the false negatives most frequently occurred among people with physical disabilities, which is against the act utilitarian perspective, in that it did not maximize the happiness and interests for people involved, but rather leave some specific group of people at risk, after performing ethical check for every scenario (i.e., group of people in this case).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c719b",
   "metadata": {},
   "source": [
    "**Note:** this case is fictional but inspired by a real algorithm, called VioGén, used in Spain to determine the risk level of victims of gender-based violence and assign protection measures. The algorithm has been recently going under severe scrutiny [(Read more).](https://eticas.ai/the-adversarial-audit-of-viogen-three-years-later/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716871c",
   "metadata": {},
   "source": [
    "### Question 3: Write your own Black Mirror episode\n",
    "\n",
    "Now that you have acquired the necessary familiarity with some required knowledge and terminology, it is time to use your creativity!\n",
    "\n",
    "**Step 1:** Brainstorm *one* near future technology based on a topic of your choice. It should be close enough that it seems like a plausible future. Describe it in the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422e4c9",
   "metadata": {},
   "source": [
    "*Microchips are implanted directly into the human body to store personal and financial information, such as IDs, medical records, and credit card details. This technology allows easy access to services, quick payments, and enhanced convenience in everyday life, replacing wallets and physical identification.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915ce50",
   "metadata": {},
   "source": [
    "**Step 2:** What are the potential social implications and/or ethical issues and/or regulatory challenges with this technology? Explain if and how they are connected to FATE (e.g. is it a Fairness issue? Or maybe a Transparency issue? It could be more than one option)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24479bf",
   "metadata": {},
   "source": [
    "*The microchips technology might have multiple issues relate to the violation of FATE, and two of them could be:*\n",
    "\n",
    "***Fairness issue:** Not everyone can afford the advanced security versions of the chip. Vulnerable populations (e.g. lower socio-economic status group) may end up with less secure versions, making them easier targets for exploitation. Criminals could target these individuals, knowing their chips are less protected.*\n",
    "\n",
    "***Transparent issue:** Once microchips are put into practice in people's every-day life, a majority of people as users might not be aware of or be informed by how much private personal information of themselves are collected by the algorithm, given its popularity and wide usage because of the easy access.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecd0e0",
   "metadata": {},
   "source": [
    "**Step 3:** Time for storytelling! Write the summary of a new Black Mirror episode based on the technology of your choice. Try covering all of the following:\n",
    "\n",
    "- What do you think might be a cautionary tale related to this technology? \n",
    "- What fictional person in the future would best illustrate this caution? Provide a detailed description and explain what makes them the best character to carry your message.\n",
    "- What is their story? Explain their background, their motivations, and their journey through your episode.\n",
    "\n",
    "As part of your submission, please update the episode thumbnail slide (from Module 1 slide deck) using information from your episode. Don't forget to add a picture! Then, share it with the rest of the class on [Canvas](https://canvas.ubc.ca/courses/134264/discussion_topics/1943920).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b3d7b",
   "metadata": {},
   "source": [
    "- *Cautionary Tale: In the near future, people have microchips implanted in their bodies to store personal information like IDs, medical records, and credit card details. This makes daily life more convenient, as people no longer need to carry wallets or identification. However, a dark side of this technology begins to emerge. Criminals start targeting individuals with less secure chips, knowing they are easier to hack. They violently steal body parts where the chips are implanted to extract sensitive information, causing fear and panic throughout society.*\n",
    "- *Fictional Charactor: John, a 35-year-old delivery driver from a working-class background, uses a basic, less-secured chip for payments and ID. As crimes targeting chip users rise, he becomes anxious, knowing he cannot afford better protection. He represents those vulnerable due to financial limitations.*\n",
    "- *Story: John, the 35-year-old delivery driver, lives in fear as news spreads about criminals attacking people with microchips implanted in their wrists. The criminals steal personal information by cutting off the part of the body where the chip is placed. Most of the victims are people with low incomes who cannot afford a more secure version of the chips, like John. However, since the microchips ease people's every-day life, almost every place and store refuses to take transactions using traditional methods, and people on the relatively low socio-economics status like John have no choice but to be forced to use it to maintain a normal life, due to the development of technology. John’s fear grows worse when his uncle, who is also from a working-class background, is attacked. The criminals cut his wrist to take the chip, and he dies from losing too much blood. This is the first reported death from chip-related crimes. The tragedy triggers public outrage, sparking protests and riots fueled by anger over the unequal security of the technology and the government’s failure to protect vulnerable groups. As the situation gets more dangerous, John realizes he is a potential target. He cannot afford to upgrade his chip, so he thinks about joining the underground network to illegally remove his chip altogether, despite the physical life risks and not being to live normally in a futuristic modern city. John must decide whether to live in constant fear or take a dangerous step to protect himself.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c018b",
   "metadata": {},
   "source": [
    "**Step 4:** Let's take a step back, and imagine that you are (one of) an activist/legistlator/technology practitioner at the time the technology described in your episode is being developed and its use being discussed. Select *one* of the ethical perspectives listed in [this reading](https://www.scu.edu/ethics-in-technology-practice/ethical-lenses/), and use this perspective to argue against its deployment. Pay particular attention to the counter-arguments! People with interests in this technology will certainly argue against you, and you must anticipate and rebut their claims. Include at least 2 conter-arguments and how you would respond to them.\n",
    "\n",
    "**Ethical perspective chosen: the Justice/Fairness Perspective** \n",
    "\n",
    "**Argument: The use of microchips is unfair because the benefits and risks are not shared equally. People from lower socioeconomic groups are more at risk since they can’t afford the secure versions of the chips. While wealthier individuals enjoy the benefits of security and convenience, poorer people face a higher chance of being targeted by criminals, making the technology unjust.**\n",
    "\n",
    "**First counter-argument (with rebuttal):**\n",
    "\n",
    "**- Counter-argument: Some might say that everyone, even lower-income people, benefits from the convenience of microchips. The technology makes life easier for everyone, regardless of their financial situation.**\n",
    "\n",
    "**- Rebuttal: Although convenience improves for everyone, the risks are much greater for lower-income people. Wealthier individuals can protect themselves with better security, but poorer people face higher risks without that option. This makes the system unfair.**\n",
    "\n",
    "**Second counter-argument (with rebuttal):**\n",
    "\n",
    "**- Counter-argument: Even people in high positions can be targeted because their information is valuable, so they aren’t completely safe either.**\n",
    "\n",
    "**- Rebuttal: While high-status individuals may be targeted, they can afford better protection, which lowers their risks. Poorer individuals have no way to defend themselves, making them easier and more frequent targets. This imbalance is what makes the system unjust.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a531a70",
   "metadata": {},
   "source": [
    "**Step 5:** Finally, let's end on a more positive note and imagine a \"Light Mirror\" scenario, where the negative consequences of the technology you have described are averted and positive results are achieved in their stead. Try answering the following questions:\n",
    "\n",
    "1. What kinds of solutions can be deployed in the immediate for addressing the harms of the technology you have described? What could we do to ensure that we don’t get to the negative consequences you imagined later in the future?\n",
    "2. Could you imagine a scenario where the technology you have described is used with positive consequences, given the appropriate safe guards? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa8a9e",
   "metadata": {},
   "source": [
    "*1.*\n",
    "\n",
    "*There might be a couple immediate solutions to address the harms of microchips. One would be letting the government provide funds specifically to support the lower income people to upgrade their microchips based on their annual tax filings, just so every citizen regardless of their socioeconomic background could obtain the same level of privacy protection in the application of this technology, and therefore, the fairness issue would be solved since everyone involved in this technology has been treated equally. At the same time, the government should also have a part of the funding to encourage the public service sectors (such as stores, hospitals, police offices, or businesses) to continue using traditional methods for transactions at the same time, just so people are able to choose between opt-in microchips or stay the same as before.*\n",
    "\n",
    "*The other would be create user-centered informed consent or terms and conditions for users to easily understand the benefits and risks, to make the technology more transparent to public. At the same time, a smooth opt-out system should also be provided by the microchip manufacturer to ensure the users could withdraw safely and in a timely manner whenever they would like to discontinue. To ensure we do not get to the negative consequences we imagined, having specific laws and guidelines to ensure harsh punishments applied towards chip-related crimes would also be helpful to decrease the incidences of such crimes.*\n",
    "\n",
    "*2.* \n",
    "\n",
    "*With the help of the government-supported funding, every citizen could afford to upgrade their microchips to the high-level privacy-protected ones now. People like John, who developed PTSD symptoms by watching their families get attacked and become a victim of chip crimes, can also live a carefree life without worrying the traditional method of transaction being completely substituted. If users are unsatisfied with the microchip service, raise safety concerns, or have any emergency situation while using the service, with the newly developed fast opt-out system by the manufacturer, they could also get an immediate opt-out in less than a minute, which prevents their information being hacked/violently attacked and robbed by criminals. One time, John's brother Jason is walking on the street and he notices someone is approaching him with an intent to attack him violently and rob his microchips. He quickly decided to opt-out and clean his information out of the chip system. He also reported the criminal characteristics to the police shortly after. Thank to the systematic law guidelines, the criminal gets punishments accordingly and the news spread among the publics.*\n",
    "\n",
    "*With the speedy and highly safe methods of accessing personal information, youth people learned to apply microchips in their every-day life without the need of carrying all their IDs and files included sensitive privacy information. It also decreases the report of identification or physical credit card lost. For senior people, once they get used to the microchip services, such technology eliminates the barrier of them not being able to learn to use mobile payment like Apple Pay or PayPal. Instead, all they need to use is their own wrist with chips embedded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44908a97",
   "metadata": {},
   "source": [
    "### Sources\n",
    "The Black Mirror Writers' Room exercises was designed by [Dr. Casey Fiesler](https://caseyfiesler.com/). Links to her work and publications:\n",
    "- [The Black Mirror Writers Room: The Case (and Caution) for Ethical Speculation in CS Education](https://medium.com/cuinfoscience/the-black-mirror-writers-room-the-case-and-caution-for-ethical-speculation-in-cs-education-5c81d05d2c67)\n",
    "- [\"Run Wild a Little With Your Imagination\": Ethical Speculation in Computing Education with Black Mirror](https://dl.acm.org/doi/abs/10.1145/3478431.3499308)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2f627",
   "metadata": {},
   "source": [
    "# Python Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f514",
   "metadata": {},
   "source": [
    "In this section of the assignment, we will review useful Python functions and libraries that will allow you to read and analyze data, as well as training simple Machine Learning models. \n",
    "\n",
    "## Section 1: Exploring datasets with Pandas\n",
    "\n",
    "First, we will need a dataset to work on. Let's use a [weather type dataset](https://www.kaggle.com/datasets/nikhil7280/weather-type-classification), a good starting dataset (we will save more interesting cases for later!). Download this dataset from the link to use it for this exercise.\n",
    "\n",
    "We also need to import the necessary library to read and manipulate our dataset, which is [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html). The imports are given to you. Next, use the `read_csv()` function to import the data in your workspace. The documentation for this function can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84cc57bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Cloud Cover</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Season</th>\n",
       "      <th>Visibility (km)</th>\n",
       "      <th>Location</th>\n",
       "      <th>Weather Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>73</td>\n",
       "      <td>9.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>1010.82</td>\n",
       "      <td>2</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.5</td>\n",
       "      <td>inland</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>96</td>\n",
       "      <td>8.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>1011.43</td>\n",
       "      <td>7</td>\n",
       "      <td>Spring</td>\n",
       "      <td>10.0</td>\n",
       "      <td>inland</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.0</td>\n",
       "      <td>64</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1018.72</td>\n",
       "      <td>5</td>\n",
       "      <td>Spring</td>\n",
       "      <td>5.5</td>\n",
       "      <td>mountain</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>83</td>\n",
       "      <td>1.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1026.25</td>\n",
       "      <td>7</td>\n",
       "      <td>Spring</td>\n",
       "      <td>1.0</td>\n",
       "      <td>coastal</td>\n",
       "      <td>Sunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.0</td>\n",
       "      <td>74</td>\n",
       "      <td>17.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>overcast</td>\n",
       "      <td>990.67</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2.5</td>\n",
       "      <td>mountain</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n",
       "0         14.0        73         9.5               82.0  partly cloudy   \n",
       "1         39.0        96         8.5               71.0  partly cloudy   \n",
       "2         30.0        64         7.0               16.0          clear   \n",
       "3         38.0        83         1.5               82.0          clear   \n",
       "4         27.0        74        17.0               66.0       overcast   \n",
       "\n",
       "   Atmospheric Pressure  UV Index  Season  Visibility (km)  Location  \\\n",
       "0               1010.82         2  Winter              3.5    inland   \n",
       "1               1011.43         7  Spring             10.0    inland   \n",
       "2               1018.72         5  Spring              5.5  mountain   \n",
       "3               1026.25         7  Spring              1.0   coastal   \n",
       "4                990.67         1  Winter              2.5  mountain   \n",
       "\n",
       "  Weather Type  \n",
       "0        Rainy  \n",
       "1       Cloudy  \n",
       "2        Sunny  \n",
       "3        Sunny  \n",
       "4        Rainy  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"weather_classification_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af59a2ab",
   "metadata": {},
   "source": [
    "Now, let's use the [`describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function of the Pandas library to get an overview of the dataset, and answer the following questions (you can write your answers in this box):\n",
    "\n",
    "- What is the maximum temperature recorded in the dataset? *109.000000*\n",
    "- What is the average wind speed? *9.832197*\n",
    "\n",
    "Note: some of the values you will see may appear unrealistic (such as incredibly high temperatures). The dataset is artificially generated and purposefully includes outliers to practice detection and handling, but it is not something we will worry about it this exercise - we are just interested in getting some practice with useful commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f3ab972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Visibility (km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13200.000000</td>\n",
       "      <td>13200.000000</td>\n",
       "      <td>13200.000000</td>\n",
       "      <td>13200.000000</td>\n",
       "      <td>13200.000000</td>\n",
       "      <td>13200.000000</td>\n",
       "      <td>13200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.127576</td>\n",
       "      <td>68.710833</td>\n",
       "      <td>9.832197</td>\n",
       "      <td>53.644394</td>\n",
       "      <td>1005.827896</td>\n",
       "      <td>4.005758</td>\n",
       "      <td>5.462917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.386327</td>\n",
       "      <td>20.194248</td>\n",
       "      <td>6.908704</td>\n",
       "      <td>31.946541</td>\n",
       "      <td>37.199589</td>\n",
       "      <td>3.856600</td>\n",
       "      <td>3.371499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>800.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>994.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1007.650000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1016.772500</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>109.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>1199.210000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Temperature      Humidity    Wind Speed  Precipitation (%)  \\\n",
       "count  13200.000000  13200.000000  13200.000000       13200.000000   \n",
       "mean      19.127576     68.710833      9.832197          53.644394   \n",
       "std       17.386327     20.194248      6.908704          31.946541   \n",
       "min      -25.000000     20.000000      0.000000           0.000000   \n",
       "25%        4.000000     57.000000      5.000000          19.000000   \n",
       "50%       21.000000     70.000000      9.000000          58.000000   \n",
       "75%       31.000000     84.000000     13.500000          82.000000   \n",
       "max      109.000000    109.000000     48.500000         109.000000   \n",
       "\n",
       "       Atmospheric Pressure      UV Index  Visibility (km)  \n",
       "count          13200.000000  13200.000000     13200.000000  \n",
       "mean            1005.827896      4.005758         5.462917  \n",
       "std               37.199589      3.856600         3.371499  \n",
       "min              800.120000      0.000000         0.000000  \n",
       "25%              994.800000      1.000000         3.000000  \n",
       "50%             1007.650000      3.000000         5.000000  \n",
       "75%             1016.772500      7.000000         7.500000  \n",
       "max             1199.210000     14.000000        20.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985c0a8",
   "metadata": {},
   "source": [
    "The `describe()` function is helpful, but it does not answer all the questions we may have. For example, we did not get any idea about the class distribution in our dataset, that is, how many samples we have for each of the four classes (Rainy, Cloudy, Sunny, Snowy). Can you write a line of code to answer this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a752e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              count\n",
      "Weather Type       \n",
      "Rainy          3300\n",
      "Cloudy         3300\n",
      "Sunny          3300\n",
      "Snowy          3300\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Weather Type\"].value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ababd",
   "metadata": {},
   "source": [
    "Thanks to `describe()`, we know that the minimum temperature recorded is -25 C, but we have no idea which sample it belongs to. Can you write a line of code to find the sample number and also the Weather Type associated to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f73283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4609    Snowy\n",
      "Name: Weather Type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[df[\"Temperature\"]==df[\"Temperature\"].min()][\"Weather Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346446a",
   "metadata": {},
   "source": [
    "Again thanks to `describe()`, we know that 25% of the samples in the dataset have a recorded Precipitation higher that 82 (you can verify this in the output table), but how many of these are Snowy? Answer this question in 1 line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebea834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243\n"
     ]
    }
   ],
   "source": [
    "print(len(df[(df['Precipitation (%)'] > 82)&(df['Weather Type']=='Snowy')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75a00a",
   "metadata": {},
   "source": [
    "Finally, sometimes we may be interested in sorting the dataframe by the values in a column. In this cell, sort the dataframe by humidity in descending order, and check the results by printing the first 5 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07aaa619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Cloud Cover</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Season</th>\n",
       "      <th>Visibility (km)</th>\n",
       "      <th>Location</th>\n",
       "      <th>Weather Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>29.0</td>\n",
       "      <td>109</td>\n",
       "      <td>21.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>1018.98</td>\n",
       "      <td>9</td>\n",
       "      <td>Winter</td>\n",
       "      <td>7.5</td>\n",
       "      <td>inland</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>16.0</td>\n",
       "      <td>109</td>\n",
       "      <td>27.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>overcast</td>\n",
       "      <td>1007.30</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>11.5</td>\n",
       "      <td>inland</td>\n",
       "      <td>Cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9707</th>\n",
       "      <td>51.0</td>\n",
       "      <td>109</td>\n",
       "      <td>17.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>overcast</td>\n",
       "      <td>994.03</td>\n",
       "      <td>8</td>\n",
       "      <td>Spring</td>\n",
       "      <td>5.5</td>\n",
       "      <td>coastal</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>16.0</td>\n",
       "      <td>109</td>\n",
       "      <td>39.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>1011.38</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2.0</td>\n",
       "      <td>inland</td>\n",
       "      <td>Rainy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12566</th>\n",
       "      <td>4.0</td>\n",
       "      <td>109</td>\n",
       "      <td>16.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>overcast</td>\n",
       "      <td>988.15</td>\n",
       "      <td>12</td>\n",
       "      <td>Winter</td>\n",
       "      <td>3.5</td>\n",
       "      <td>inland</td>\n",
       "      <td>Snowy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n",
       "1303          29.0       109        21.0               93.0  partly cloudy   \n",
       "8716          16.0       109        27.0              102.0       overcast   \n",
       "9707          51.0       109        17.0               98.0       overcast   \n",
       "2812          16.0       109        39.0               87.0  partly cloudy   \n",
       "12566          4.0       109        16.0               93.0       overcast   \n",
       "\n",
       "       Atmospheric Pressure  UV Index  Season  Visibility (km) Location  \\\n",
       "1303                1018.98         9  Winter              7.5   inland   \n",
       "8716                1007.30         1  Winter             11.5   inland   \n",
       "9707                 994.03         8  Spring              5.5  coastal   \n",
       "2812                1011.38        11  Spring              2.0   inland   \n",
       "12566                988.15        12  Winter              3.5   inland   \n",
       "\n",
       "      Weather Type  \n",
       "1303        Cloudy  \n",
       "8716        Cloudy  \n",
       "9707         Rainy  \n",
       "2812         Rainy  \n",
       "12566        Snowy  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(ascending=False, by=\"Humidity\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f01f7e",
   "metadata": {},
   "source": [
    "As last step of this section, save the sorted dataframe in a new csv file called \"weather_data_by_humidity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c81a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(ascending=False, by=\"Humidity\").to_csv(\"weather_data_by_humidity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097333c3",
   "metadata": {},
   "source": [
    "## Section 2: Training ML models with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b407cb",
   "metadata": {},
   "source": [
    "We are now interested in creating a model to predict the weather type based on the features available. Let's see how to do that using the python library [Scikit-learn](https://scikit-learn.org/stable/), while reviewing some important concepts about training and evaluating models. Simply run the cells below to see the output and answer the related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cefd1",
   "metadata": {},
   "source": [
    "First, we need to split our data set into training and testing set. The next cell shows how to do that. We will also separate the Weather Type column (target) from the other columns (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9133334b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Cloud Cover</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Season</th>\n",
       "      <th>Visibility (km)</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12987</th>\n",
       "      <td>26.0</td>\n",
       "      <td>45</td>\n",
       "      <td>3.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1011.01</td>\n",
       "      <td>7</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>5.0</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>29.0</td>\n",
       "      <td>71</td>\n",
       "      <td>21.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>1013.77</td>\n",
       "      <td>12</td>\n",
       "      <td>Winter</td>\n",
       "      <td>6.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5590</th>\n",
       "      <td>38.0</td>\n",
       "      <td>63</td>\n",
       "      <td>5.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1013.87</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>7.5</td>\n",
       "      <td>mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>17.0</td>\n",
       "      <td>66</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>992.22</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>32.0</td>\n",
       "      <td>39</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>1021.43</td>\n",
       "      <td>9</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>5.5</td>\n",
       "      <td>inland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Temperature  Humidity  Wind Speed  Precipitation (%)    Cloud Cover  \\\n",
       "12987         26.0        45         3.5               10.0          clear   \n",
       "905           29.0        71        21.0               86.0  partly cloudy   \n",
       "5590          38.0        63         5.5               11.0          clear   \n",
       "7269          17.0        66        18.0               63.0  partly cloudy   \n",
       "1417          32.0        39         7.5                3.0          clear   \n",
       "\n",
       "       Atmospheric Pressure  UV Index  Season  Visibility (km)  Location  \n",
       "12987               1011.01         7  Autumn              5.0    inland  \n",
       "905                 1013.77        12  Winter              6.5    inland  \n",
       "5590                1013.87        11  Spring              7.5  mountain  \n",
       "7269                 992.22         1  Winter              2.5    inland  \n",
       "1417                1021.43         9  Autumn              5.5    inland  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)  # 80%-20% train test split on df\n",
    "\n",
    "X_train, y_train = train_df.drop(columns=[\"Weather Type\"]), train_df[\"Weather Type\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"Weather Type\"]), test_df[\"Weather Type\"]\n",
    "\n",
    "X_train.head()  # quick visual check on X_train, the features dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91b843",
   "metadata": {},
   "source": [
    "**Question for you:** creating a testing set is very important when training a model. **Why? How is it used? What would happen if we did not do this very important step?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab435e4",
   "metadata": {},
   "source": [
    "*Creating a test set is important because it allows us to ensure that the model is generalizable to unseen data. The test set acts as a new dataset and helps determine whether the model performs well on information it hasn’t seen before. Without a test set, if we only used the training data to evaluate the model, it could lead to overfitting, where the model performs well on the training data but fails to generalize to new data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cf61e",
   "metadata": {},
   "source": [
    "As you can see, the dataset includes categorical features. Most classifiers require categorical features to be transformed before they can be used for training and prediction. The code below uses [One Hot Encoding](https://www.geeksforgeeks.org/ml-one-hot-encoding/) to convert the categorical features Cloud Cover, Season and Location, while leaving the numberical features unchanged.\n",
    "\n",
    "This is a simple example of data preprocessing. Preprocessing can be more extensive (for example, including [scaling of numerical features](https://www.geeksforgeeks.org/ml-feature-scaling-part-2/)), but we are only interested in an overview of the fundamentals, so we will just apply One Hot Encoding to make the data usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e61d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "passthrough = ['Temperature', 'Humidity', 'Wind Speed', 'Precipitation (%)', \n",
    "               'Atmospheric Pressure', 'UV Index', 'Visibility (km)']\n",
    "categorical = ['Cloud Cover', 'Season', 'Location']\n",
    "\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical),  # OHE on categorical features\n",
    "    (\"passthrough\", passthrough),  # no transformations on the numberical features\n",
    ")\n",
    "\n",
    "# Fit the encoder on the training data and transform\n",
    "train_encoded = ct.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "test_encoded = ct.transform(X_test)\n",
    "\n",
    "# Convert the encoded data back to DataFrame for better readability\n",
    "\n",
    "column_names = (\n",
    "    ct.named_transformers_[\"onehotencoder\"].get_feature_names_out().tolist() + passthrough\n",
    ")\n",
    "\n",
    "X_train_encoded = pd.DataFrame(train_encoded, columns=column_names)\n",
    "X_test_encoded = pd.DataFrame(test_encoded, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a98bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cloud Cover_clear</th>\n",
       "      <th>Cloud Cover_cloudy</th>\n",
       "      <th>Cloud Cover_overcast</th>\n",
       "      <th>Cloud Cover_partly cloudy</th>\n",
       "      <th>Season_Autumn</th>\n",
       "      <th>Season_Spring</th>\n",
       "      <th>Season_Summer</th>\n",
       "      <th>Season_Winter</th>\n",
       "      <th>Location_coastal</th>\n",
       "      <th>Location_inland</th>\n",
       "      <th>Location_mountain</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Visibility (km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1011.01</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1013.77</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1013.87</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>992.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1021.43</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10555</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>929.72</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1016.64</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10557</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1014.45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10558</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1018.10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10559</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1008.30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10560 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Cloud Cover_clear  Cloud Cover_cloudy  Cloud Cover_overcast  \\\n",
       "0                    1.0                 0.0                   0.0   \n",
       "1                    0.0                 0.0                   0.0   \n",
       "2                    1.0                 0.0                   0.0   \n",
       "3                    0.0                 0.0                   0.0   \n",
       "4                    1.0                 0.0                   0.0   \n",
       "...                  ...                 ...                   ...   \n",
       "10555                0.0                 0.0                   0.0   \n",
       "10556                0.0                 0.0                   1.0   \n",
       "10557                1.0                 0.0                   0.0   \n",
       "10558                1.0                 0.0                   0.0   \n",
       "10559                0.0                 0.0                   0.0   \n",
       "\n",
       "       Cloud Cover_partly cloudy  Season_Autumn  Season_Spring  Season_Summer  \\\n",
       "0                            0.0            1.0            0.0            0.0   \n",
       "1                            1.0            0.0            0.0            0.0   \n",
       "2                            0.0            0.0            1.0            0.0   \n",
       "3                            1.0            0.0            0.0            0.0   \n",
       "4                            0.0            1.0            0.0            0.0   \n",
       "...                          ...            ...            ...            ...   \n",
       "10555                        1.0            0.0            1.0            0.0   \n",
       "10556                        0.0            0.0            0.0            1.0   \n",
       "10557                        0.0            0.0            0.0            1.0   \n",
       "10558                        0.0            0.0            0.0            1.0   \n",
       "10559                        1.0            0.0            0.0            1.0   \n",
       "\n",
       "       Season_Winter  Location_coastal  Location_inland  Location_mountain  \\\n",
       "0                0.0               0.0              1.0                0.0   \n",
       "1                1.0               0.0              1.0                0.0   \n",
       "2                0.0               0.0              0.0                1.0   \n",
       "3                1.0               0.0              1.0                0.0   \n",
       "4                0.0               0.0              1.0                0.0   \n",
       "...              ...               ...              ...                ...   \n",
       "10555            0.0               0.0              1.0                0.0   \n",
       "10556            0.0               0.0              0.0                1.0   \n",
       "10557            0.0               0.0              1.0                0.0   \n",
       "10558            0.0               1.0              0.0                0.0   \n",
       "10559            0.0               0.0              0.0                1.0   \n",
       "\n",
       "       Temperature  Humidity  Wind Speed  Precipitation (%)  \\\n",
       "0             26.0      45.0         3.5               10.0   \n",
       "1             29.0      71.0        21.0               86.0   \n",
       "2             38.0      63.0         5.5               11.0   \n",
       "3             17.0      66.0        18.0               63.0   \n",
       "4             32.0      39.0         7.5                3.0   \n",
       "...            ...       ...         ...                ...   \n",
       "10555        -19.0      27.0         5.5               66.0   \n",
       "10556         27.0      63.0         8.0               73.0   \n",
       "10557         25.0      23.0         4.5               13.0   \n",
       "10558         38.0      39.0         9.0               10.0   \n",
       "10559         11.0      61.0         9.5               41.0   \n",
       "\n",
       "       Atmospheric Pressure  UV Index  Visibility (km)  \n",
       "0                   1011.01       7.0              5.0  \n",
       "1                   1013.77      12.0              6.5  \n",
       "2                   1013.87      11.0              7.5  \n",
       "3                    992.22       1.0              2.5  \n",
       "4                   1021.43       9.0              5.5  \n",
       "...                     ...       ...              ...  \n",
       "10555                929.72       9.0             14.5  \n",
       "10556               1016.64       2.0              1.0  \n",
       "10557               1014.45      11.0              6.5  \n",
       "10558               1018.10      10.0              7.0  \n",
       "10559               1008.30       3.0              6.5  \n",
       "\n",
       "[10560 rows x 18 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to see what the encoded data set looks like\n",
    "\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a80f0",
   "metadata": {},
   "source": [
    "**Question for you:** It appears that we applied the same One Hot Encoding transformation to both training and test set. Why did we bother doing this operation on the separate sets? Could have we just transformed the original dataframe `df`, and then split it in training and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca55587",
   "metadata": {},
   "source": [
    "*We apply OHE separately to the training and test sets to prevent data leakage, which could lead to overly optimistic evaluation results. If the model is exposed to data it should not have seen during training, it may perform better than it actually should.  Therefore, we cannot simply transform the original dataframe before splitting, as it would lead to unreliable evaluation results.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcfe025",
   "metadata": {},
   "source": [
    "There are many classifiers we can choose from. We will use [Decision Trees](https://scikit-learn.org/stable/modules/svm.html) to start. Decision trees are very simple classification algorithms, although they have typically mediocre performance on complex classification problems.\n",
    "\n",
    "A certain level of familiarity with Decision Trees is expected in this course. You may want to review the material from your previous courses, or this [introduction](https://www.youtube.com/watch?v=ZVR2Way4nwQ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d713bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier() # Create a decision tree\n",
    "model.fit(X_train_encoded, y_train) # Fit a decision tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404049a",
   "metadata": {},
   "source": [
    "Now that we have the tree, we want to see how well it performs. Let's first check the accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05252e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train_encoded, y_train) # Score the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4344e6",
   "metadata": {},
   "source": [
    "**100% accuracy!!!**\n",
    "\n",
    "...\n",
    "\n",
    "This sounds too good to be true... let's check the test set to see how well the trees perform on unseen samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "023977ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128787878787878"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_encoded, y_test) # Score the decision tree on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de26d80",
   "metadata": {},
   "source": [
    "Accuracy dropped significantly when we moved to unseen samples!\n",
    "\n",
    "This is because the Decision Tree, if left unsupervised, is very prone to **overfitting**. \n",
    "\n",
    "**Question for you:** what does it mean for a model to overfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5f27c",
   "metadata": {},
   "source": [
    "*Overfitting happens when a model learns the training data too well, including unnecessary details. This makes the model perform well on training data but poorly on unseen data. It can't generalize properly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b305fe8",
   "metadata": {},
   "source": [
    "To prevent a model from overfitting, we tune its **hyperparameters.** Hyperparameters are like knobs that we can use to regulate the way a model learn. \n",
    "\n",
    "Some hyperparameters for the scikit-learn DecisionTreeClassifier include:\n",
    "- max_depth: the maximum distance between the root node and a leaf node\n",
    "- min_samples_split: the minimum number of samples required to split an internal node\n",
    "- min_samples_leaf; the minimum number of samples required to be at a leaf node\n",
    "\n",
    "You can look up other hyperparameters and their default values in the DecisionTreeClassifier [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). By default, the maximum depth value is set to *None*, that is, the tree is free to grow until it has parfectly classified all samples. As we have seen, this results in perfect accuracy on the training set, but much lower accuracy on unseen samples. \n",
    "\n",
    "Run the cell below to see the depth of our overfitted tree:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f3a24c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a045a20",
   "metadata": {},
   "source": [
    "If we could find the right depth for our tree, we could reduce the problem of overfitting.\n",
    "\n",
    "**Question for you:** what would happen if we reduce the depth of the tree *too much*? What do you expect the accuracy on training and test set to look like in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21765a",
   "metadata": {},
   "source": [
    "*If we reduce the depth of the decision tree too much, the model will underfit. This causes lower accuracy on both the training and test sets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b918627",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is typically done on a **validation set.** A validation set is a set of samples not used for training, like the test set, but unlike the test set, we are allowed to use this multiple times as we look for the best hyperparameter values.\n",
    "\n",
    "Because our data set is rather small, it is not great to take more samples from the training set to create a validation set, because:\n",
    "- We would have fewer samples (less information) to train our model\n",
    "- The validation set would also be small, and result in a highly variable accuracy measure (meaning if we run the experiment again changing the samples in each set, we will likely get very different results)\n",
    "\n",
    "There is a method that we can use to eliminate both problems, called ***k*-fold cross-validation.** Cross-validation iteratively separates training and validation set (*k* times), so we get multiple measures of accuracy on the validation sets, which can be averaged for a more stable result. A good understanding of how cross-validation works is important for any data scientist. I encourage you to review cross-validation from previous courses, or this [introduction video](https://www.youtube.com/watch?v=4cv8VYonepA) (courtesy of Dr. Kolhatkar).\n",
    "\n",
    "Scikit-learn has a great method that we can use to perform cross-validation and find the best hyperparameters for a model at the same time, called [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). Let's use it to find the best depth for our Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff5e951e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "             return_train_score=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "             return_train_score=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(), n_jobs=-1,\n",
       "             param_grid={'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np  # to create the array of values for depth\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": np.arange(1, 20, 1)  # testing all depths from 1 to 19\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model, param_grid, cv=10, n_jobs=-1, return_train_score=True   # 10-fold cross-validation for all possible \n",
    "                                                                   # depths\n",
    ")\n",
    "grid_search.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4668c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9115530303030303"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8304fb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f4143",
   "metadata": {},
   "source": [
    "**Complete the sentence (replace --?--):** Among all possible trees, GridSearchCV picked a tree of depth **12**, with an average validation accuracy of **0.9115530303030303**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc8999",
   "metadata": {},
   "source": [
    "The accuracy on the training set is no longer 100%, but we expect this tree to perform better on unseen samples. Let's try it on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9db5c74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9128787878787878"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "best_tree.score(X_test_encoded, y_test) # Score the decision tree on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301822",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The accuracy is similar to when the model was overfitting, but hyperparameter tuning brought us 2 advantages:\n",
    "- We had a more realistic expectation of what our accuracy was going to be (closer to 91%, not 100%)\n",
    "- We simplified the model and reduced its depth. This makes the model faster and easier to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5209b3",
   "metadata": {},
   "source": [
    "**Question for you:** on what samples (or portion of samples) of `X_train_encoded` was the final model (`best_tree`) trained on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdded8",
   "metadata": {},
   "source": [
    "*The final model was trained on the entire training set (= `X_train_encoded`)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc9bf",
   "metadata": {},
   "source": [
    "The model can now be used to get predictions for unseen samples. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a6e51b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cloud Cover_clear</th>\n",
       "      <th>Cloud Cover_cloudy</th>\n",
       "      <th>Cloud Cover_overcast</th>\n",
       "      <th>Cloud Cover_partly cloudy</th>\n",
       "      <th>Season_Autumn</th>\n",
       "      <th>Season_Spring</th>\n",
       "      <th>Season_Summer</th>\n",
       "      <th>Season_Winter</th>\n",
       "      <th>Location_coastal</th>\n",
       "      <th>Location_inland</th>\n",
       "      <th>Location_mountain</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Precipitation (%)</th>\n",
       "      <th>Atmospheric Pressure</th>\n",
       "      <th>UV Index</th>\n",
       "      <th>Visibility (km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1015.42</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cloud Cover_clear  Cloud Cover_cloudy  Cloud Cover_overcast  \\\n",
       "2005                1.0                 0.0                   0.0   \n",
       "\n",
       "      Cloud Cover_partly cloudy  Season_Autumn  Season_Spring  Season_Summer  \\\n",
       "2005                        0.0            0.0            0.0            0.0   \n",
       "\n",
       "      Season_Winter  Location_coastal  Location_inland  Location_mountain  \\\n",
       "2005            1.0               0.0              0.0                1.0   \n",
       "\n",
       "      Temperature  Humidity  Wind Speed  Precipitation (%)  \\\n",
       "2005         33.0      67.0         5.5                5.0   \n",
       "\n",
       "      Atmospheric Pressure  UV Index  Visibility (km)  \n",
       "2005               1015.42      10.0              8.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sample = X_test_encoded.sample(n=1, random_state=42)\n",
    "\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09c18eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sunny'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tree.predict(random_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a95af",
   "metadata": {},
   "source": [
    "# Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadac478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1) If you have completed this assignment in a group, please write a detailed description of how you divided the work and how you helped each other completing it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba472e70",
   "metadata": {},
   "source": [
    "We did Question 1 together in class. And then Ayuho finished the rest everything but Step 5 of Question 3. Muhan finished Q3 Step 5 and did cross-check for Ayuho's answer, providing additional portion of answers for Q2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c81ffd",
   "metadata": {},
   "source": [
    "2) Have you used ChatGPT or a similar Large Language Model (LLM) to complete this homework? Please describe how you used the tool. **We will never deduct points for using LLMs for completing homework assignments,** but this helps us understand how you are using the tool and advise you in case we believe you are using it incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fa1e6",
   "metadata": {},
   "source": [
    "We used ChatGPT to help us brainstorm the general idea of potential solutions to the harms in Q3 Step 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7bf06",
   "metadata": {},
   "source": [
    "3) Have you struggled with some parts (or all) of this homework? Do you have pending questions you would like to ask? Write them down here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b82cbc",
   "metadata": {},
   "source": [
    "Not for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126aa526-7b15-4c56-a083-46e31c660b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci430]",
   "language": "python",
   "name": "conda-env-dsci430-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
